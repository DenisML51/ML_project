{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "import logging \n",
    "\n",
    "import torch \n",
    "import mlflow \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils \n",
    "import torch.nn as nn \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USER'] = 'Denis Rusinov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\train-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\train-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flask.cli import F\n",
    "\n",
    "\n",
    "train_mnist = torchvision.datasets.MNIST('data_mnist/train', train=True, \n",
    "                                         transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                         download=True)\n",
    "val_mnist = torchvision.datasets.MNIST('data_mnist/test', train=False, \n",
    "                                       transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                       download=True)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_mnist,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_mnist,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, prob, n_inside):\n",
    "        super(FCNetwork, self).__init__() \n",
    "        self.fc1 = nn.Linear(784, n_inside)\n",
    "        self.fc2 = nn.Linear(n_inside, 10)\n",
    "        self.fc1_act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = prob)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28)\n",
    "        y = self.fc1(self.dropout(x))\n",
    "        y = self.fc1_act(y)\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/16 16:06:17 INFO mlflow.tracking.fluent: Experiment with name 'PyTorch_test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Rusinov.DS/PycharmProjects/ML_project/dns_purchase/artefacts/980200151464093923', creation_time=1734329177469, experiment_id='980200151464093923', last_update_time=1734329177469, lifecycle_stage='active', name='PyTorch_test', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"PyTorch_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð²Ñ‹Ð²Ð¾Ð´ Ð²Ð¾Ñ€Ð½Ð¸Ð½Ð³Ð¾Ð² Ð¾Ñ‚ MLflow\n",
    "mlflow_logger = logging.getLogger(\"mlflow\")\n",
    "mlflow_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "prob = 0.15\n",
    "n_inside = 50\n",
    "lr = 1e-3\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸\n",
    "def accuracy(y_pred, labels):\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 1 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 1 Epoch of network learning is over:\n",
      "Train results Epoch 1: Train loss - 0.6602, Train accuracy - 0.8283\n",
      "Validation results Epoch 1: Val loss - 0.3185, Test accuracy - 0.9098\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 2 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 2 Epoch of network learning is over:\n",
      "Train results Epoch 2: Train loss - 0.3177, Train accuracy - 0.9091\n",
      "Validation results Epoch 2: Val loss - 0.2610, Test accuracy - 0.9266\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 3 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 3 Epoch of network learning is over:\n",
      "Train results Epoch 3: Train loss - 0.2723, Train accuracy - 0.9224\n",
      "Validation results Epoch 3: Val loss - 0.2274, Test accuracy - 0.9357\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 4 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 4 Epoch of network learning is over:\n",
      "Train results Epoch 4: Train loss - 0.2362, Train accuracy - 0.9324\n",
      "Validation results Epoch 4: Val loss - 0.1945, Test accuracy - 0.9444\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 5 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 5 Epoch of network learning is over:\n",
      "Train results Epoch 5: Train loss - 0.2074, Train accuracy - 0.9402\n",
      "Validation results Epoch 5: Val loss - 0.1734, Test accuracy - 0.9517\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 6 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 6 Epoch of network learning is over:\n",
      "Train results Epoch 6: Train loss - 0.1833, Train accuracy - 0.9467\n",
      "Validation results Epoch 6: Val loss - 0.1519, Test accuracy - 0.9572\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 7 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 7 Epoch of network learning is over:\n",
      "Train results Epoch 7: Train loss - 0.1645, Train accuracy - 0.9516\n",
      "Validation results Epoch 7: Val loss - 0.1384, Test accuracy - 0.9610\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 8 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 8 Epoch of network learning is over:\n",
      "Train results Epoch 8: Train loss - 0.1485, Train accuracy - 0.9569\n",
      "Validation results Epoch 8: Val loss - 0.1295, Test accuracy - 0.9631\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 9 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 9 Epoch of network learning is over:\n",
      "Train results Epoch 9: Train loss - 0.1376, Train accuracy - 0.9601\n",
      "Validation results Epoch 9: Val loss - 0.1228, Test accuracy - 0.9660\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 10 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 10 Epoch of network learning is over:\n",
      "Train results Epoch 10: Train loss - 0.1264, Train accuracy - 0.9625\n",
      "Validation results Epoch 10: Val loss - 0.1141, Test accuracy - 0.9683\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 11 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 11 Epoch of network learning is over:\n",
      "Train results Epoch 11: Train loss - 0.1197, Train accuracy - 0.9648\n",
      "Validation results Epoch 11: Val loss - 0.1075, Test accuracy - 0.9702\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 12 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 12 Epoch of network learning is over:\n",
      "Train results Epoch 12: Train loss - 0.1123, Train accuracy - 0.9662\n",
      "Validation results Epoch 12: Val loss - 0.1021, Test accuracy - 0.9712\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 13 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 13 Epoch of network learning is over:\n",
      "Train results Epoch 13: Train loss - 0.1054, Train accuracy - 0.9686\n",
      "Validation results Epoch 13: Val loss - 0.1005, Test accuracy - 0.9702\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 14 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 14 Epoch of network learning is over:\n",
      "Train results Epoch 14: Train loss - 0.0986, Train accuracy - 0.9707\n",
      "Validation results Epoch 14: Val loss - 0.0990, Test accuracy - 0.9714\n",
      "Saving model because its better\n",
      "---\n",
      "ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ 15 ÑÐ¿Ð¾Ñ…Ð¸\n",
      "The 15 Epoch of network learning is over:\n",
      "Train results Epoch 15: Train loss - 0.0945, Train accuracy - 0.9716\n",
      "Validation results Epoch 15: Val loss - 0.0970, Test accuracy - 0.9723\n",
      "Saving model because its better\n",
      "---\n",
      "max accuracy =  0.9723\n",
      "ðŸƒ View run FCNetwork_1 at: http://localhost:5000/#/experiments/980200151464093923/runs/5315c1ddcc40415ba0c560d9b6850c45\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/980200151464093923\n"
     ]
    }
   ],
   "source": [
    "# ÐÐ°Ñ‡Ð°Ð»Ð¾ MLflow Ð·Ð°Ð¿ÑƒÑÐºÐ°\n",
    "with mlflow.start_run(run_name='FCNetwork_1') as run:\n",
    "    model = FCNetwork(prob=prob, n_inside=n_inside)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    mlflow.log_param(\"prob dropout\", prob)\n",
    "    mlflow.log_param(\"neurons 2 layer\", n_inside)\n",
    "    mlflow.log_param(\"lr\", lr)\n",
    "    mlflow.log_param(\"optimizer\", 'Adam')\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "    maxacc = 0\n",
    "    itr_record = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        print(f'ÐÐ°Ñ‡Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ {epoch} ÑÐ¿Ð¾Ñ…Ð¸')\n",
    "        for itr, data in enumerate(train_loader):\n",
    "            imgs = data[0].to(device)  # [B, H, W]\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            y_pred = model(imgs) \n",
    "            loss = loss_func(y_pred, labels)\n",
    "\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "            train_acc += accuracy(y_pred, labels) * imgs.size(0)\n",
    "            train_samples += imgs.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "        print(f'The {epoch} Epoch of network learning is over:')\n",
    "        print(f'Train results Epoch {epoch}: Train loss - {train_loss:.4f}, Train accuracy - {train_acc:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for itr, data in enumerate(val_loader):\n",
    "                imgs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "                y_pred = model(imgs)\n",
    "                loss = loss_func(y_pred, labels)\n",
    "\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                val_acc += accuracy(y_pred, labels) * imgs.size(0)\n",
    "                val_samples += imgs.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc /= val_samples\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "        print(f'Validation results Epoch {epoch}: Val loss - {val_loss:.4f}, Test accuracy - {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > maxacc:\n",
    "            print('Saving model because its better')\n",
    "            maxacc = val_acc\n",
    "            mlflow.pytorch.log_model(model, \"model\")\n",
    "        print('---')\n",
    "\n",
    "    print('max accuracy = ', maxacc)\n",
    "    mlflow.log_metric(\"max val accuracy\", maxacc)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
