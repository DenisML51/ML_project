{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "import logging \n",
    "\n",
    "import torch \n",
    "import mlflow \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils \n",
    "import torch.nn as nn \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USER'] = 'Denis Rusinov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\train-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data_mnist/train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\train-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/test\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data_mnist/test\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flask.cli import F\n",
    "\n",
    "\n",
    "train_mnist = torchvision.datasets.MNIST('data_mnist/train', train=True, \n",
    "                                         transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                         download=True)\n",
    "val_mnist = torchvision.datasets.MNIST('data_mnist/test', train=False, \n",
    "                                       transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                       download=True)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_mnist,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_mnist,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, prob, n_inside):\n",
    "        super(FCNetwork, self).__init__() \n",
    "        self.fc1 = nn.Linear(784, n_inside)\n",
    "        self.fc2 = nn.Linear(n_inside, 10)\n",
    "        self.fc1_act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = prob)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28)\n",
    "        y = self.fc1(self.dropout(x))\n",
    "        y = self.fc1_act(y)\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/16 16:06:17 INFO mlflow.tracking.fluent: Experiment with name 'PyTorch_test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Rusinov.DS/PycharmProjects/ML_project/dns_purchase/artefacts/980200151464093923', creation_time=1734329177469, experiment_id='980200151464093923', last_update_time=1734329177469, lifecycle_stage='active', name='PyTorch_test', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"PyTorch_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã–≤–æ–¥ –≤–æ—Ä–Ω–∏–Ω–≥–æ–≤ –æ—Ç MLflow\n",
    "mlflow_logger = logging.getLogger(\"mlflow\")\n",
    "mlflow_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "prob = 0.15\n",
    "n_inside = 50\n",
    "lr = 1e-3\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "def accuracy(y_pred, labels):\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 1 —ç–ø–æ—Ö–∏\n",
      "The 1 Epoch of network learning is over:\n",
      "Train results Epoch 1: Train loss - 0.6602, Train accuracy - 0.8283\n",
      "Validation results Epoch 1: Val loss - 0.3185, Test accuracy - 0.9098\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 2 —ç–ø–æ—Ö–∏\n",
      "The 2 Epoch of network learning is over:\n",
      "Train results Epoch 2: Train loss - 0.3177, Train accuracy - 0.9091\n",
      "Validation results Epoch 2: Val loss - 0.2610, Test accuracy - 0.9266\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 3 —ç–ø–æ—Ö–∏\n",
      "The 3 Epoch of network learning is over:\n",
      "Train results Epoch 3: Train loss - 0.2723, Train accuracy - 0.9224\n",
      "Validation results Epoch 3: Val loss - 0.2274, Test accuracy - 0.9357\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 4 —ç–ø–æ—Ö–∏\n",
      "The 4 Epoch of network learning is over:\n",
      "Train results Epoch 4: Train loss - 0.2362, Train accuracy - 0.9324\n",
      "Validation results Epoch 4: Val loss - 0.1945, Test accuracy - 0.9444\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 5 —ç–ø–æ—Ö–∏\n",
      "The 5 Epoch of network learning is over:\n",
      "Train results Epoch 5: Train loss - 0.2074, Train accuracy - 0.9402\n",
      "Validation results Epoch 5: Val loss - 0.1734, Test accuracy - 0.9517\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 6 —ç–ø–æ—Ö–∏\n",
      "The 6 Epoch of network learning is over:\n",
      "Train results Epoch 6: Train loss - 0.1833, Train accuracy - 0.9467\n",
      "Validation results Epoch 6: Val loss - 0.1519, Test accuracy - 0.9572\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 7 —ç–ø–æ—Ö–∏\n",
      "The 7 Epoch of network learning is over:\n",
      "Train results Epoch 7: Train loss - 0.1645, Train accuracy - 0.9516\n",
      "Validation results Epoch 7: Val loss - 0.1384, Test accuracy - 0.9610\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 8 —ç–ø–æ—Ö–∏\n",
      "The 8 Epoch of network learning is over:\n",
      "Train results Epoch 8: Train loss - 0.1485, Train accuracy - 0.9569\n",
      "Validation results Epoch 8: Val loss - 0.1295, Test accuracy - 0.9631\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 9 —ç–ø–æ—Ö–∏\n",
      "The 9 Epoch of network learning is over:\n",
      "Train results Epoch 9: Train loss - 0.1376, Train accuracy - 0.9601\n",
      "Validation results Epoch 9: Val loss - 0.1228, Test accuracy - 0.9660\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 10 —ç–ø–æ—Ö–∏\n",
      "The 10 Epoch of network learning is over:\n",
      "Train results Epoch 10: Train loss - 0.1264, Train accuracy - 0.9625\n",
      "Validation results Epoch 10: Val loss - 0.1141, Test accuracy - 0.9683\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 11 —ç–ø–æ—Ö–∏\n",
      "The 11 Epoch of network learning is over:\n",
      "Train results Epoch 11: Train loss - 0.1197, Train accuracy - 0.9648\n",
      "Validation results Epoch 11: Val loss - 0.1075, Test accuracy - 0.9702\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 12 —ç–ø–æ—Ö–∏\n",
      "The 12 Epoch of network learning is over:\n",
      "Train results Epoch 12: Train loss - 0.1123, Train accuracy - 0.9662\n",
      "Validation results Epoch 12: Val loss - 0.1021, Test accuracy - 0.9712\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 13 —ç–ø–æ—Ö–∏\n",
      "The 13 Epoch of network learning is over:\n",
      "Train results Epoch 13: Train loss - 0.1054, Train accuracy - 0.9686\n",
      "Validation results Epoch 13: Val loss - 0.1005, Test accuracy - 0.9702\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 14 —ç–ø–æ—Ö–∏\n",
      "The 14 Epoch of network learning is over:\n",
      "Train results Epoch 14: Train loss - 0.0986, Train accuracy - 0.9707\n",
      "Validation results Epoch 14: Val loss - 0.0990, Test accuracy - 0.9714\n",
      "Saving model because its better\n",
      "---\n",
      "–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ 15 —ç–ø–æ—Ö–∏\n",
      "The 15 Epoch of network learning is over:\n",
      "Train results Epoch 15: Train loss - 0.0945, Train accuracy - 0.9716\n",
      "Validation results Epoch 15: Val loss - 0.0970, Test accuracy - 0.9723\n",
      "Saving model because its better\n",
      "---\n",
      "max accuracy =  0.9723\n",
      "üèÉ View run FCNetwork_1 at: http://localhost:5000/#/experiments/980200151464093923/runs/5315c1ddcc40415ba0c560d9b6850c45\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/980200151464093923\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—á–∞–ª–æ MLflow –∑–∞–ø—É—Å–∫–∞\n",
    "with mlflow.start_run(run_name='FCNetwork_1') as run:\n",
    "    model = FCNetwork(prob=prob, n_inside=n_inside)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    mlflow.log_param(\"prob dropout\", prob)\n",
    "    mlflow.log_param(\"neurons 2 layer\", n_inside)\n",
    "    mlflow.log_param(\"lr\", lr)\n",
    "    mlflow.log_param(\"optimizer\", 'Adam')\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "    maxacc = 0\n",
    "    itr_record = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        print(f'–ù–∞—á–∞–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ {epoch} —ç–ø–æ—Ö–∏')\n",
    "        for itr, data in enumerate(train_loader):\n",
    "            imgs = data[0].to(device)  # [B, H, W]\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            y_pred = model(imgs) \n",
    "            loss = loss_func(y_pred, labels)\n",
    "\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "            train_acc += accuracy(y_pred, labels) * imgs.size(0)\n",
    "            train_samples += imgs.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "        print(f'The {epoch} Epoch of network learning is over:')\n",
    "        print(f'Train results Epoch {epoch}: Train loss - {train_loss:.4f}, Train accuracy - {train_acc:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for itr, data in enumerate(val_loader):\n",
    "                imgs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "                y_pred = model(imgs)\n",
    "                loss = loss_func(y_pred, labels)\n",
    "\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                val_acc += accuracy(y_pred, labels) * imgs.size(0)\n",
    "                val_samples += imgs.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc /= val_samples\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "        print(f'Validation results Epoch {epoch}: Val loss - {val_loss:.4f}, Test accuracy - {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > maxacc:\n",
    "            print('Saving model because its better')\n",
    "            maxacc = val_acc\n",
    "            mlflow.pytorch.log_model(model, \"model\")\n",
    "        print('---')\n",
    "\n",
    "    print('max accuracy = ', maxacc)\n",
    "    mlflow.log_metric(\"max val accuracy\", maxacc)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpmdarima\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_arima\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n",
      "File \u001b[1;32mc:\\Users\\Rusinov.DS\\PycharmProjects\\ML_project\\.venv\\Lib\\site-packages\\pmdarima\\__init__.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[0;32m     54\u001b[0m     tsdisplay\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32mc:\\Users\\Rusinov.DS\\PycharmProjects\\ML_project\\.venv\\Lib\\site-packages\\pmdarima\\arima\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapprox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Rusinov.DS\\PycharmProjects\\ML_project\\.venv\\Lib\\site-packages\\pmdarima\\arima\\approx.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# R approx function\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m c, check_endog\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_callable\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n",
      "File \u001b[1;32mc:\\Users\\Rusinov.DS\\PycharmProjects\\ML_project\\.venv\\Lib\\site-packages\\pmdarima\\utils\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Rusinov.DS\\PycharmProjects\\ML_project\\.venv\\Lib\\site-packages\\pmdarima\\utils\\array.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m C_intgrt_vec\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas_series\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_iterable\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     23\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_series\u001b[39m(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32mc:\\Users\\Rusinov.DS\\PycharmProjects\\ML_project\\.venv\\Lib\\site-packages\\pmdarima\\utils\\_array.pyx:1\u001b[0m, in \u001b[0;36minit pmdarima.utils._array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "data =  pd.read_csv('data_month.csv')\n",
    "\n",
    "forecast_df = pd.DataFrame(columns=['month_date', 'category_id', 'final_count_forecast', 'date_load'])\n",
    "for i in data.category_id.unique():\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"sarima_test\")\n",
    "\n",
    "    # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã–≤–æ–¥ –≤–æ—Ä–Ω–∏–Ω–≥–æ–≤ –æ—Ç MLflow\n",
    "    mlflow_logger = logging.getLogger(\"mlflow\")\n",
    "    mlflow_logger.setLevel(logging.ERROR)\n",
    "\n",
    "    with mlflow.start_run(run_name=f'sarima_count_{i}'):\n",
    "\n",
    "        df = data.copy()\n",
    "\n",
    "        df = df.where(df.category_id == i).dropna()\n",
    "\n",
    "        a = df.sort_values('date')\n",
    "\n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df.sort_index()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        df.product_count = df.product_count.astype(float)\n",
    "        df.product_cost = df.product_cost.astype(float)\n",
    "        df = df.asfreq('MS')\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        train_data = df.iloc[:-2]\n",
    "\n",
    "        if len(train_data) < 12:\n",
    "            print(\n",
    "                f\"‚ùå–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}. \"\n",
    "                f\"–î–ª–∏–Ω–∞ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(train_data)}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        test_start_date = df.index[-2]\n",
    "        test_end_date = test_start_date + pd.DateOffset(\n",
    "            months=len(train_data) + 25\n",
    "        )\n",
    "        test_dates = pd.date_range(test_start_date, test_end_date, freq=\"MS\")\n",
    "\n",
    "        val_data = df.iloc[-2:]\n",
    "\n",
    "        n_jobs = 18\n",
    "\n",
    "        mlflow.log_param('test_start_date', test_start_date)\n",
    "        mlflow.log_param('test_end_date', test_end_date)\n",
    "        mlflow.log_param('n_jobs', n_jobs)\n",
    "\n",
    "        model_count = auto_arima(train_data.product_count, seasonal=True, m=12, stepwise=False , trace=False, n_jobs=n_jobs)\n",
    "        forecast_count = model_count.predict(n_periods=len(train_data) + 26)\n",
    "\n",
    "\n",
    "        for j in range(len(forecast_count)):\n",
    "            if forecast_count.iloc[j] < 0:\n",
    "                forecast_count.iloc[j] = 0\n",
    "\n",
    "        if len(forecast_count.unique()) < len(train_data.product_count) / 2:\n",
    "            continue\n",
    "\n",
    "\n",
    "        forecast = pd.DataFrame(\n",
    "            {\n",
    "                \"date\": test_dates,\n",
    "                \"category_id\": i,\n",
    "                \"final_count_forecast\": forecast_count,\n",
    "                'date_load': pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "            }\n",
    "        )\n",
    "\n",
    "        val_set = pd.merge(val_data, forecast, on='month_date', how='inner')\n",
    "        mlflow.log_metric(mean_absolute_error(val_set['product_count'], val_set['final_count_forecast']))\n",
    "\n",
    "\n",
    "\n",
    "        forecast_df = pd.concat([forecast_df, forecast])\n",
    "        print(f\"‚úÖ–ü—Ä–æ–≥–Ω–æ–∑ –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}\")\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:cmd: where.exe tbb.dll\n",
      "cwd: None\n",
      "DEBUG:cmdstanpy:TBB already found in load path\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\Rusinov.DS\\AppData\\Local\\Temp\\tmpfu6e266z\\830ej6ty.json\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\Rusinov.DS\\AppData\\Local\\Temp\\tmpfu6e266z\\98ob5_1j.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['C:\\\\Users\\\\Rusinov.DS\\\\PycharmProjects\\\\ML_project\\\\.venv\\\\Lib\\\\site-packages\\\\prophet\\\\stan_model\\\\prophet_model.bin', 'random', 'seed=14658', 'data', 'file=C:\\\\Users\\\\Rusinov.DS\\\\AppData\\\\Local\\\\Temp\\\\tmpfu6e266z\\\\830ej6ty.json', 'init=C:\\\\Users\\\\Rusinov.DS\\\\AppData\\\\Local\\\\Temp\\\\tmpfu6e266z\\\\98ob5_1j.json', 'output', 'file=C:\\\\Users\\\\Rusinov.DS\\\\AppData\\\\Local\\\\Temp\\\\tmpfu6e266z\\\\prophet_modeluztwik6k\\\\prophet_model-20241216170737.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
      "17:07:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:07:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run prophet_count_2c281485-37d2-11e4-8d43-00155d031202 at: http://localhost:5000/#/experiments/964258905629541857/runs/971700fbc4b74fea870309ea9a8fd0a7\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/964258905629541857\n",
      "‚ùå–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 2c281485-37d2-11e4-8d43-00155d031202: 'ds'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from prophet import Prophet\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('C:/Users/Rusinov.DS/PycharmProjects/ML_project/dns_purchase/dataset/data_month.csv')\n",
    "data = data.sort_values('date')\n",
    "dates = data['date'].unique()\n",
    "min_date = dates[-2]\n",
    "print(min_date)\n",
    "\n",
    "forecast_df = pd.DataFrame(columns=['date', 'category_id', 'final_count_forecast', 'final_cost_forecast', 'date_load'])\n",
    "for i in data.category_id.unique():\n",
    "    try:\n",
    "        df = data.copy()\n",
    "\n",
    "        mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "        mlflow.set_experiment(\"prophet_test\")\n",
    "\n",
    "        # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã–≤–æ–¥ –≤–æ—Ä–Ω–∏–Ω–≥–æ–≤ –æ—Ç MLflow\n",
    "        mlflow_logger = logging.getLogger(\"mlflow\")\n",
    "        mlflow_logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "        with mlflow.start_run(run_name=f'prophet_count_{i}'):\n",
    "\n",
    "            df = df.where(df.category_id == i).dropna()\n",
    "\n",
    "            a = df.sort_values('date')\n",
    "\n",
    "            df.set_index('date', inplace=True)\n",
    "            df = df.sort_index()\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "            df.product_count = df.product_count.astype(float)\n",
    "            df.product_cost = df.product_cost.astype(float)\n",
    "            df = df.asfreq('MS')\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            train_data = df.iloc[:-2]\n",
    "\n",
    "            if len(train_data) < 12:\n",
    "                print(\n",
    "                    f\"‚ùå–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}. \"\n",
    "                    f\"–î–ª–∏–Ω–∞ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(train_data)}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            test_start_date = df.index[-2]\n",
    "            test_end_date = test_start_date + pd.DateOffset(\n",
    "                months=25\n",
    "            )\n",
    "            test_dates = pd.date_range(test_start_date, test_end_date, freq=\"MS\")\n",
    "\n",
    "            if test_dates.min() < pd.to_datetime(min_date):\n",
    "                print(f\"‚ùå–£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}.\")\n",
    "                continue\n",
    "\n",
    "            m1 = Prophet()\n",
    "\n",
    "            count_data = pd.DataFrame(\n",
    "                {\n",
    "                'ds' : pd.to_datetime(train_data.index),\n",
    "                'y': train_data['product_count']\n",
    "                })\n",
    "\n",
    "            count_data = count_data.sort_values('ds')\n",
    "\n",
    "            m1.fit(count_data)\n",
    "\n",
    "            future_count = m1.make_future_dataframe(periods=26, freq=\"MS\")\n",
    "            forecast_count = m1.predict(future_count)\n",
    "\n",
    "            m1.plot_components(forecast)\n",
    "\n",
    "\n",
    "\n",
    "            # mlflow.log_metric()\n",
    "            mlflow\n",
    "        mlflow.end_run()\n",
    "\n",
    "        with mlflow.start_run(run_name=f'prophet_cost_{i}'):\n",
    "\n",
    "            df = df.where(df.category_id == i).dropna()\n",
    "\n",
    "            a = df.sort_values('date')\n",
    "\n",
    "            df.set_index('date', inplace=True)\n",
    "            df = df.sort_index()\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "            df.product_count = df.product_count.astype(float)\n",
    "            df.product_cost = df.product_cost.astype(float)\n",
    "            df = df.asfreq('MS')\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            train_data = df.iloc[:-2]\n",
    "\n",
    "            if len(train_data) < 12:\n",
    "                print(\n",
    "                    f\"‚ùå–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}. \"\n",
    "                    f\"–î–ª–∏–Ω–∞ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(train_data)}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            test_start_date = df.index[-2]\n",
    "            test_end_date = test_start_date + pd.DateOffset(\n",
    "                months=25\n",
    "            )\n",
    "            test_dates = pd.date_range(test_start_date, test_end_date, freq=\"MS\")\n",
    "\n",
    "            if test_dates.min() < pd.to_datetime(min_date):\n",
    "                print(f\"‚ùå–£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}.\")\n",
    "                continue\n",
    "\n",
    "            m2 = Prophet()\n",
    "            cost_data = pd.DataFrame(\n",
    "                {\n",
    "                'ds' : pd.to_datetime(train_data.index),\n",
    "                'y': train_data['product_cost']\n",
    "                })\n",
    "            cost_data = cost_data.sort_values('ds')\n",
    "\n",
    "            m2.fit(cost_data)\n",
    "\n",
    "            future_cost = m2.make_future_dataframe(periods=26, freq=\"MS\")\n",
    "            forecast_cost = m2.predict(future_cost)\n",
    "\n",
    "\n",
    "            forecast = pd.DataFrame(\n",
    "                {\n",
    "                    \"month_date\": forecast_count.iloc[-26:]['ds'],\n",
    "                    \"category_id\": i,\n",
    "                    \"final_count_forecast\": forecast_count.iloc[-26:]['yhat'].apply(lambda x: 0 if x < 0 else x),\n",
    "                    \"final_cost_forecast\": forecast_cost.iloc[-26:]['yhat'].apply(lambda x: 0 if x < 0 else x),\n",
    "                    'date_load': pd.to_datetime(pd.Timestamp.now().strftime(\"%Y-%m-%d\"))\n",
    "                }\n",
    "            )\n",
    "        mlflow.end_run()\n",
    "\n",
    "\n",
    "        forecast_df = pd.concat([forecast_df, forecast])\n",
    "        print(f\"‚úÖ–ü—Ä–æ–≥–Ω–æ–∑ –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {i}: {e}\")\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
